{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1102,
   "id": "957589d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# import gym\n",
    "\n",
    "# env = gym.make('CartPole-v1') \n",
    "\"\"\" \n",
    "Observation:\n",
    "    Type: Box(4)\n",
    "    Num     Observation               Min                     Max\n",
    "    0       Cart Position             -4.8                    4.8\n",
    "    1       Cart Velocity             -Inf                    Inf\n",
    "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "    3       Pole Angular Velocity     -Inf                    Inf\n",
    "Actions:\n",
    "    Type: Discrete(2)\n",
    "    Num   Action\n",
    "    0     Push cart to the left\n",
    "    1     Push cart to the right\n",
    "    Note: The amount the velocity that is reduced or increased is not\n",
    "    fixed; it depends on the angle the pole is pointing. This is because\n",
    "    the center of gravity of the pole increases the amount of energy needed\n",
    "    to move the cart underneath it\n",
    "Reward:\n",
    "    Reward is 1 for every step taken, including the termination step\n",
    "Starting State:\n",
    "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "Episode Termination:\n",
    "    Pole Angle is more than 12 degrees.\n",
    "    Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "    the display).\n",
    "    Episode length is greater than 200.\n",
    "    Solved Requirements:\n",
    "    Considered solved when the average return is greater than or equal to\n",
    "    195.0 over 100 consecutive trials.\n",
    "\"\"\"\n",
    "\n",
    "X_range = [-4.8, 4.8]\n",
    "v_range = [-100, 100]#[-100000, 100000] #[float('-inf'), float('inf')]\n",
    "theta_range = [-24, 24]\n",
    "\n",
    "#TODO: Try different values for anglev_range\n",
    "anglev_range = [-10, 10] #[-100000, 100000]#[float('-inf'), float('inf')]\n",
    "start_range = [-0.05, 0.05]\n",
    "\n",
    "terminating_cond =[2.4, 12, 200]\n",
    "\n",
    "action_set = [0,1] #left, right\n",
    "\n",
    "M = 3 # dimensionality of the fourier transform\n",
    "softmax_sigma = 1 #0.1\n",
    "# gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "id": "8e95edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_radian(ang):\n",
    "    return ang*np.pi/180\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "id": "df9e6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(action, x, x_dot, theta, theta_dot):\n",
    "    gravity = 9.8\n",
    "    masscart = 1.0\n",
    "    masspole = 0.1\n",
    "    total_mass = masspole + masscart\n",
    "    length = 0.5  # actually half the pole's length\n",
    "    polemass_length = masspole * length\n",
    "    force_mag = 10.0\n",
    "    tau = 0.02\n",
    "\n",
    "    force = force_mag if action == 1 else -force_mag\n",
    "    costheta = np.cos(theta) # theta in radians\n",
    "    sintheta = np.sin(theta)\n",
    "\n",
    "    # from gym https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "    temp = (force + polemass_length * theta_dot ** 2 * sintheta) / total_mass\n",
    "    thetaacc = (gravity * sintheta - costheta * temp) / (length * (4.0 / 3.0 - masspole * costheta ** 2 / total_mass))\n",
    "    xacc = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "    \n",
    "    #euler\n",
    "    x = x + tau * x_dot\n",
    "    x_dot = x_dot + tau * xacc\n",
    "    theta = theta + tau * theta_dot\n",
    "    theta_dot = theta_dot + tau * thetaacc\n",
    "    \n",
    "    #semi euler\n",
    "    # x_dot = x_dot + tau * xacc\n",
    "    # x = x + tau * x_dot\n",
    "    # theta_dot = theta_dot + tau * thetaacc\n",
    "    # theta = theta + tau * theta_dot\n",
    "\n",
    "    return x, x_dot, theta, theta_dot\n",
    "    \n",
    "def is_terminating(x, x_dot, theta, theta_dot, step):\n",
    "    if x <= -terminating_cond[0] or x >= terminating_cond[0] or theta <= -in_radian(terminating_cond[1]) or theta >= in_radian(terminating_cond[1]) or step>=terminating_cond[2]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def reward(x, x_dot, theta, theta_dot, step):\n",
    "    if is_terminating(x, x_dot, theta, theta_dot, step):\n",
    "        return 0\n",
    "    return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "id": "553be124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, x_dot, theta, theta_dot, cosineflag=True):\n",
    "    if cosineflag:\n",
    "        x = (x-X_range[0])/(X_range[1]-X_range[0])\n",
    "        theta = (theta-theta_range[0])/(theta_range[1]-theta_range[0])\n",
    "        x_dot = (x_dot - v_range[0])/(v_range[1] - v_range[0])\n",
    "        theta_dot = (theta_dot - anglev_range[0])/(anglev_range[1] - anglev_range[0])\n",
    "        \n",
    "    else:\n",
    "        x = 2*(x-X_range[0])/(X_range[1]-X_range[0]) -1\n",
    "        theta = 2*(theta-theta_range[0])/(theta_range[1]-theta_range[0]) -1\n",
    "        x_dot = 2*(x_dot - v_range[0])/(v_range[1] - v_range[0]) -1\n",
    "        theta_dot = 2*(theta_dot - anglev_range[0])/(anglev_range[1] - anglev_range[0]) -1\n",
    "\n",
    "    return  x, x_dot, theta, theta_dot\n",
    "\n",
    "def fourier(x, x_dot, theta, theta_dot, cosineflag=True): #4M+1 features\n",
    "    #normalize\n",
    "    x, x_dot, theta, theta_dot = normalize(x, x_dot, theta, theta_dot, cosineflag)\n",
    "    phi = [1]\n",
    "    if cosineflag:\n",
    "        for i in range(1, M+1):\n",
    "            phi.append(np.cos(i*np.pi*x))\n",
    "        for i in range(1, M+1):\n",
    "            phi.append(np.cos(i*np.pi*x_dot))\n",
    "        for i in range(1, M+1):\n",
    "            phi.append(np.cos(i*np.pi*theta))\n",
    "        for i in range(1, M+1):\n",
    "            phi.append(np.cos(i*np.pi*theta_dot))\n",
    "    else:\n",
    "        for i in range(1, M+1):\n",
    "            phi.append(np.sin(i*np.pi*x))\n",
    "        for i in range(1, M+1):\n",
    "            phi.append(np.sin(i*np.pi*x_dot))\n",
    "        for i in range(1, M+1):\n",
    "            phi.append(np.sin(i*np.pi*theta))\n",
    "        for i in range(1, M+1):\n",
    "            phi.append(np.sin(i*np.pi*theta_dot))\n",
    "    return np.array(phi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "id": "1eca53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_action(policy_params, x, x_dot, theta, theta_dot):\n",
    "    \n",
    "    phi_s = fourier(x, x_dot, theta, theta_dot) # (4M+1, )\n",
    "    # print(policy_params.shape, phi_s.shape)\n",
    "    \n",
    "    policy_val = np.dot(policy_params.T, phi_s) #(4M,1) (4M+1, 2)\n",
    "    policy_exp = np.exp(softmax_sigma*policy_val)\n",
    "    \n",
    "    iter_count = 0\n",
    "    while(np.sum(policy_exp)==0):\n",
    "        print(\"Adding softmax terms.\")\n",
    "        iter_count += 1\n",
    "        policy_exp = np.exp(np.power(0.1, iter_count)*policy_val)\n",
    "    \n",
    "    print(\"Policy val: \",policy_val,\". Policy exp: \",policy_exp)\n",
    "    policy_exp /= np.sum(policy_exp)\n",
    "    \n",
    "    # print(policy_exp, x, x_dot, theta, theta_dot)\n",
    "    return policy_exp #(2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "id": "771026cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACTOR_CRITIC(alpha_w, alpha_theta, gamma=1.0):\n",
    "    policy_params = np.random.normal(0, 0.1, (4*M+1,len(action_set))) #np.ones((4*M+1,len(action_set)))*(-0.01)\n",
    "    value_params = np.ones(4*M+1)*0.01\n",
    "    episode_length, avg_return = [], []\n",
    "    total_iterations = 10000\n",
    "    for iter in range(total_iterations):\n",
    "        policy_params_temp = policy_params.copy()\n",
    "        #run episode\n",
    "        #initial state\n",
    "        x = np.random.uniform(start_range[0], start_range[1])\n",
    "        theta = np.random.uniform(start_range[0], start_range[1])\n",
    "        x_dot  = np.random.uniform(start_range[0], start_range[1])\n",
    "        theta_dot = np.random.uniform(start_range[0], start_range[1])\n",
    "        step = 1\n",
    "        _return = 0\n",
    "        \n",
    "        # #using gym\n",
    "        # x, x_dot, theta, theta_dot = env.reset() \n",
    "\n",
    "        #run epsidoe\n",
    "        while not is_terminating(x, x_dot, theta, theta_dot, step):\n",
    "            #choose action\n",
    "            curr_action = random.choices(action_set, softmax_action(policy_params, x, x_dot, theta, theta_dot))[0]\n",
    "#             curr_action = np.argmax(softmax_action(policy_params, x, x_dot, theta, theta_dot))\n",
    "            # #using gym\n",
    "            # observation, curr_reward, done, info = env.step(curr_action)\n",
    "            # next_x, next_x_dot, next_theta, mext_theta_dot = observation\n",
    "\n",
    "            #next state\n",
    "            next_x, next_x_dot, next_theta, mext_theta_dot = transition(curr_action, x, x_dot, theta, theta_dot)\n",
    "            #reward\n",
    "            curr_reward = reward(next_x, next_x_dot, next_theta, mext_theta_dot, step)\n",
    "            _return += curr_reward*gamma**(step-1)\n",
    "            step += 1\n",
    "#             print(x, x_dot, theta, theta_dot, curr_action, softmax_action(policy_params, x, x_dot, theta, theta_dot))\n",
    "\n",
    "            phi_s = fourier(x, x_dot, theta, theta_dot)\n",
    "            phi_next_s = fourier(next_x, next_x_dot, next_theta, mext_theta_dot)\n",
    "            if not is_terminating(next_x, next_x_dot, next_theta, mext_theta_dot, step):\n",
    "                delta = curr_reward +gamma*np.dot(phi_next_s, value_params) - np.dot(phi_s, value_params)\n",
    "            else:\n",
    "                delta = curr_reward - np.dot(phi_s, value_params)\n",
    "                \n",
    "#             print(\"Delta: \",delta)\n",
    "            #update value params\n",
    "            alpha_temp = min(alpha_w, np.abs(1/np.dot(phi_s, gamma*phi_next_s - phi_s)))\n",
    "        \n",
    "            value_params += alpha_temp*delta*phi_s\n",
    "            #update policy params\n",
    "            policy = softmax_action(policy_params, x, x_dot, theta, theta_dot)\n",
    "\n",
    "            if curr_action == 0:\n",
    "                policy_params[:,0] += alpha_theta*delta*(1-policy[0])*phi_s\n",
    "                policy_params[:,1] += alpha_theta*delta*(-1*policy[0])*phi_s\n",
    "                # print(curr_action, delta, policy)\n",
    "            if curr_action == 1:\n",
    "                policy_params[:,0] += alpha_theta*delta*(-policy[1])*phi_s\n",
    "                policy_params[:,1] += alpha_theta*delta*(1-policy[1])*phi_s\n",
    "            print(curr_action, delta, policy)\n",
    "            x, x_dot, theta, theta_dot = next_x, next_x_dot, next_theta, mext_theta_dot\n",
    "\n",
    "        episode_length.append(step)\n",
    "        avg_return.append(_return)\n",
    "        \n",
    "        mean_episode_length = np.mean(avg_return[max(0, iter-100): iter+1])\n",
    "        print(\"alphaTheta: \",alpha_theta,\". alphaW:\", alpha_w, \"\\n EPISODE LENGTH: \",step, \"CURR ITER: \", iter, \". Mean episode len:\",mean_episode_length)\n",
    "        if mean_episode_length > 195.0:\n",
    "            print(\"Hooray... solved\")\n",
    "            break\n",
    "        max_diff = np.max(np.abs(policy_params_temp - policy_params))\n",
    "        print(\" Max diff: \",max_diff)\n",
    "        if max_diff/alpha_theta < 0.001: # 0.001 works with 1e-6 policy_step\n",
    "            break\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(avg_return)), avg_return)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Avg. return')\n",
    "    plt.savefig('graph_cartpole_actorcritic_cosine_cri_'+str(int(alpha_w*1e5))+'_act_'+str(int(alpha_theta*1e5))+'divBy1e5_'+str(int(total_iterations)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c314ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO: FIX COSINE OR SINE BEFORE STARTING!!!!!!!!\n",
    "\n",
    "# BEST COMBO: 2e-1, 1e-3 for cosine.\n",
    "\n",
    "alpha_w_range = [2e-1]# At least 1e-2. Not less. At most 7e-2. Not more.\n",
    "\n",
    "alpha_theta_range = [2e-3] #1e-3 is the best with 2e-1 above. Freeze it for cosine features.\n",
    "# At least 4e-3, not less. At most 1e-1, not more.\n",
    "\n",
    "# 1e-1, 5e-3: has a positive slope.\n",
    "# 2e-1, 1e-3: goes till mean 72\n",
    "# reinforce params for cart pole: 1e-6, 6e-4\n",
    "\n",
    "# 5e-2, 1e-2 with sine show learning behavior.\n",
    "# 5e-2, 2e-2 with sine show learning behavior.\n",
    "# 5e-2, 9e-2 with sine show learning behavior.\n",
    "# 5e-2, 10e-2 with sine show learning behavior.\n",
    "for alpha_w in alpha_w_range:\n",
    "    for alpha_theta in alpha_theta_range:\n",
    "# alpha_w, alpha_theta = 1e-3, 1e-5 #1e-7, 5e-3\n",
    "        ACTOR_CRITIC(alpha_w, alpha_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8894e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e588cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1af83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
